#Training neural networks with TensorFlow
#In this exercise, you will train a neural network to predict whether a credit card holder will default. The features and targets you will use to train your network are available in the Python shell as borrower_features and default. You defined the weights and biases in the previous exercise.

#Note that output_layer is defined as σ(layer1∗weights2+bias2), where σ is the sigmoid activation, layer1 is a tensor of nodes for the first hidden dense layer, weight2 is a tensor of weights, and bias2 is the bias tensor.

#The trainable variables are weights1, bias1, weights2, and bias2. Additionally, the following operations have been imported for you: nn.relu() and keras.layers.Dropout().

#Instructions
#100 XP
#Apply a rectified linear unit activation function to the first layer.
#Apply 25% dropout to layer1.
#Pass the target, targets, and the predicted values, layer2, to the cross entropy loss function.
#Add the four trainable variables to var_list in the order in which they appear as arguments to loss_function().

def loss_function(weights1, bias1, weights2, bias2, features, targets):
	# Apply relu activation functions to layer 1
	layer1 = nn.relu(add(matmul(features, weights1), bias1))
    # Apply dropout
	dropout = keras.layers.Dropout(0.25)(layer1)
	layer2 = nn.sigmoid(add(matmul(dropout, weights2), bias2))
    # Pass targets and layers2 to the cross entropy loss
	return keras.losses.binary_crossentropy(targets, layer2)
  
for j in range(0, 30000, 2000):
	features, targets = borrower_features[j:j+2000, :], default[j:j+2000, :]
    # Complete the optimizer
	opt.minimize(lambda: loss_function(weights1, bias1, weights2, bias2, features, targets), var_list=[weights1, bias1, weights2, bias2])
    
print(weights1.numpy())
